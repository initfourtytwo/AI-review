# Chapter 10. Interpreting what ConvNets learn

## Table of Contents

* [Interpretability in ConvNets](#interpretability-in-convnets)
* [Visualizing Intermediate Activations](#visualizing-intermediate-activations)
* [Visualizing ConvNet Filters](#visualizing-convnet-filters)
* [Filter Visualization Loop & Postprocessing](#filter-visualization-loop--postprocessing)
* [Class Activation Maps (Grad-CAM)](#class-activation-maps-gradcam)
* [Latent Space Visualization](#latent-space-visualization)
* [Key Takeaways](#key-takeaways)

---

## Interpretability in ConvNets

* **Problem**: Understand *why* a model predicts a class (e.g., “fridge” vs “truck”), crucial in high-stakes domains (e.g., medical imaging).
* **Claim**: ConvNets aren’t total “black boxes” because their learned **visual** representations are amenable to visualization.
* **Toolkit (4 methods)**

  * **Intermediate activations**: see how layers decompose inputs.
  * **Filter visualization**: what pattern each filter “likes”.
  * **Class activation heatmaps (CAM/Grad-CAM)**: where a class signal comes from.
  * **Latent manifold visualization**: how images are organized; spot outliers/ambiguous cases.

---

## Visualizing Intermediate Activations

![intermediate activations](./images/10-01.png)

* **What**: Display feature maps (W×H×C) for conv/pooling layers given an input image.
* **Why**:

  * Early layers → **edges / simple textures**, preserve most image detail.
  * Deeper layers → **abstract parts/concepts** (e.g., “cat ear”), less raw detail, more class info.
  * **Sparsity** increases with depth (more blank channels).
* **How**:

  1. Build a **multi-output model** returning selected layer outputs.
  2. Feed a preprocessed image; collect activations.
  3. Plot each **channel** as a 2D image, optionally in a grid with standardization for contrast.
* **Key insight**: Deep nets act as an **information distillation pipeline**—filtering irrelevant details while amplifying task-relevant structure.

---

## Visualizing ConvNet Filters

* **Goal**: Synthesize an input that **maximally activates** a chosen filter (per layer).
* **Mechanism**: **Gradient ascent in input space**

  * Define loss = mean activation of target filter.
  * Update image in the direction of ∂loss/∂image.
  * **Normalize gradients** (L2) for stable steps.
* **Model**: Use **Xception** (ImageNet) via KerasHub; target e.g., `block3_sepconv1`.
* **Interpretation**:

  * Shallow filters = **edges/colors**.
  * Mid-level = **simple textures**.
  * Deep filters = **natural-image motifs** (feathers, eyes, leaves).

---

## Filter Visualization Loop & Postprocessing

![filter visualization](./images/10-02.png)

* **Loop**: Iterate filters (e.g., 64), run ~30 ascent steps, collect images.
* **Deprocessing**: Zero-center, divide by std, scale to display range [0,255], **crop borders**, cast to `uint8`.
* **Presentation**: Stitch an **n×n grid** with margins; save (`keras.utils.save_img`).
* **Practical tips**:

  * Start from small random image (0.4–0.6).
  * Moderate **learning rate** (e.g., 10.0) and **iterations** (~30) work well.
  * Avoid dead filters by checking activation magnitude.

---

## Class Activation Maps (Grad-CAM)

![grad-cam](./images/10-03.png)

* **Purpose**: Localize **where** the model “looked” to decide a class on a **specific image**.
* **Steps**:

  1. Forward to get **last conv layer output** and **predictions**.
  2. Compute **gradients** of top class score w.r.t. last conv feature map.
  3. **Global-average** grads across spatial dims → per-channel **importance weights**.
  4. Weight channels, **sum over channels** → heatmap.
  5. **Normalize** [0,1]; **resize**; **overlay** (e.g., JET colormap at α≈0.4).
* **Backends**: Provided TF, PyTorch, and JAX variants for gradient extraction.
* **Reading the map**: Bright regions = strongest contributors. Example: **elephant ears** help distinguish **African vs Indian**.

---

## Latent Space Visualization

* **Concept**: Each layer defines a **latent manifold**; nearby points = **semantically similar** inputs.
* **Procedure**:

  * Choose a layer (often **penultimate**).
  * Collect activations for a dataset.
  * **Project** high-D to **2D** (e.g., t-SNE/UMAP) to visualize clusters, **class separability**, **outliers**.
* **Caveat**: All projections **distort** some structure (like map projections); interpret patterns qualitatively.

---

## Key Takeaways

* Early layers = **edges**, deeper layers = **concepts**; activation sparsity grows with depth.
* **Gradient ascent** on inputs is a simple, powerful way to see **filter preferences**.
* **Use predict()** for values, **use model(x)** for **differentiable** pipelines.
* **Grad-CAM** = importance-weighted last-conv features → **explainable localization**.
* **Penultimate-layer embeddings** + 2D projection help **spot outliers** and **dataset issues**.
* Visual tools are not just explanatory—they guide **debugging**, **data curation**, and **model trust**.
