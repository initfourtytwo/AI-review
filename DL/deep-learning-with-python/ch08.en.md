# Chapter 8. Image classification

## Table of Contents
- [Dense Layer vs Convolution Layer](#dense-layer-vs-convolution-layer)
- [Convolutional Neural Networks (CNNs)](#convolutional-neural-networks-cnns)
- [Padding and Strides](#padding-and-strides)
- [Pooling](#pooling)
- [Overfitting and Data Augmentation](#overfitting-and-data-augmentation)
- [Transfer Learning and Pretrained Models](#transfer-learning-and-pretrained-models)
- [Feature Extraction vs Fine-tuning](#feature-extraction-vs-fine-tuning)
- [Partial Fine-tuning](#partial-fine-tuning)
- [Key Takeaways](#key-takeaways)
- [Code](#code)

---

## Dense Layer vs Convolution Layer
![conv](./images/08-01.png)

- **Dense Layer**: learns **global patterns** (involving all input features/pixels)  
- **Convolution Layer**: learns **local patterns** (small 2D regions of the input)  

→ Convnets = strong at detecting local spatial structures in images.  

---

## Convolutional Neural Networks (CNNs)
![how-convolution-works](./images/08-02.png)

- **Translation invariance**: learned features are stable even if shifted spatially  
- **Spatial hierarchies**: can combine simple features (edges, textures) into higher-level concepts (shapes, objects)  
- **Feature map**: each filter generates a 2D activation map representing where a pattern occurs  

---

## Padding and Strides
- **Padding**  
  - `"valid"`: no padding (smaller output feature maps)  
  - `"same"`: pads input so output size = input size  
- **Strides**  
  - stride = step size of convolution window  
  - stride 1 = default, contiguous windows  
  - stride 2 = downsamples width/height by 2  

---

## Pooling
- **Max pooling**  
  - Uses 2×2 window with stride 2 → downsamples by factor of 2  
  - Keeps **maximum activation** to capture strongest presence of features  
- **Why max pooling vs strided conv or average pooling?**  
  - Max preserves strongest evidence of a pattern  
  - Average may dilute feature presence  

---

## Overfitting and Data Augmentation
- **Overfitting** = model memorizes small dataset, fails to generalize  
- **Data augmentation** = apply random transformations so model never sees the exact same image twice  
  - Increases robustness and reduces overfitting  
- Still not enough → **Dropout** added before dense classifier  

---

## Transfer Learning and Pretrained Models
- **Pretrained model** = trained on large-scale dataset (e.g., ImageNet)  
- Learned feature hierarchy can generalize to new tasks (e.g., from animals → furniture detection)  
- Key advantage: **feature portability** → useful for small-data problems  

---

## Feature Extraction vs Fine-tuning
![feature-extraction](./images/08-03.png)

- **Feature extraction**:  
  - Reuse pretrained conv base → extract features → train new dense classifier on top  
  - Pros: cheap, fast  
  - Cons: cannot use data augmentation  

- **End-to-end training**:  
  - Add dense layers to conv base → train whole model  
  - Pros: allows data augmentation  
  - Cons: computationally expensive  

- **Freezing layers** = prevents pretrained weights from being updated (to avoid destroying learned representations)  

---

## Partial Fine-tuning
- Fine-tune only the **top layers** of the conv base:  
  - Early layers → general features (edges, textures)  
  - Higher layers → task-specific features (e.g., cat ear, dog eye)  
- Fine-tuning lower layers = limited benefit + higher overfitting risk  
- Example: Xception base with 15M params → only fine-tune top layers  

---

## Key Takeaways
- Dense vs Conv: global vs local pattern learning  
- ConvNets: translation invariance + feature hierarchies  
- Padding/Stride control feature map size  
- Max pooling better than stride/average pooling for feature subsampling  
- Data augmentation combats overfitting, but limited → use with dropout  
- Transfer learning = reuse pretrained conv base → effective on small datasets  
- Feature extraction (freeze base) vs Fine-tuning (unfreeze upper layers)  
- Partial fine-tuning focuses on specialized layers to balance reuse & adaptation  

---

## Code
<details>
<summary>Show Code</summary>

```python
### 1. Environment Setup
import os
os.environ["KERAS_BACKEND"] = "jax"

from IPython.core.magic import register_cell_magic

@register_cell_magic
def backend(line, cell):
    current, required = os.environ.get("KERAS_BACKEND", ""), line.split()[-1]
    if current == required:
        get_ipython().run_cell(cell)
    else:
        print(
            f"This cell requires the {required} backend. To run it, change KERAS_BACKEND to "
            f"\"{required}\" at the top of the notebook, restart the runtime, and rerun the notebook."
        )
### 2. Download and Extract Data
import kagglehub

kagglehub.login()
download_path = kagglehub.competition_download("dogs-vs-cats")

import zipfile

with zipfile.ZipFile(download_path + "/train.zip", "r") as zip_ref:
    zip_ref.extractall(".")
### 3. Prepare Dataset (Train / Validation / Test)
import os, shutil, pathlib

original_dir = pathlib.Path("train")
new_base_dir = pathlib.Path("dogs_vs_cats_small")

def make_subset(subset_name, start_index, end_index):
    for category in ("cat", "dog"):
        dir = new_base_dir / subset_name / category
        os.makedirs(dir)
        fnames = [f"{category}.{i}.jpg" for i in range(start_index, end_index)]
        for fname in fnames:
            shutil.copyfile(src=original_dir / fname, dst=dir / fname)

make_subset("train", start_index=0, end_index=1000)
make_subset("validation", start_index=1000, end_index=1500)
make_subset("test", start_index=1500, end_index=2500)
### 4. Load Data
from keras.utils import image_dataset_from_directory

batch_size = 64
image_size = (180, 180)
train_dataset = image_dataset_from_directory(
    new_base_dir / "train", image_size=image_size, batch_size=batch_size
)
validation_dataset = image_dataset_from_directory(
    new_base_dir / "validation", image_size=image_size, batch_size=batch_size
)
test_dataset = image_dataset_from_directory(
    new_base_dir / "test", image_size=image_size, batch_size=batch_size
)
### 5. Data Augmentation
import keras
from keras import layers
import tensorflow as tf

data_augmentation_layers = [
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.2),
]

def data_augmentation(images, targets):
    for layer in data_augmentation_layers:
        images = layer(images)
    return images, targets

augmented_train_dataset = train_dataset.map(
    data_augmentation, num_parallel_calls=8
)
augmented_train_dataset = augmented_train_dataset.prefetch(tf.data.AUTOTUNE)
### 6. Load Pretrained Model (Xception)
import keras_hub

conv_base = keras_hub.models.Backbone.from_preset(
    "xception_41_imagenet",
    trainable=False,
)
preprocessor = keras_hub.layers.ImageConverter.from_preset(
    "xception_41_imagenet",
    image_size=(180, 180),
)
conv_base.trainable = False
len(conv_base.trainable_weights)
### 7. Build Model (Feature Extraction)
inputs = keras.Input(shape=(180, 180, 3))
x = preprocessor(inputs)
x = conv_base(x)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(256)(x)
x = layers.Dropout(0.25)(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs, outputs)
model.compile(
    loss="binary_crossentropy",
    optimizer="adam",
    metrics=["accuracy"],
)
### 8. Train Model (Feature Extraction)
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="feature_extraction_with_data_augmentation.keras",
        save_best_only=True,
        monitor="val_loss",
    )
]
history = model.fit(
    augmented_train_dataset,
    epochs=30,
    validation_data=validation_dataset,
    callbacks=callbacks,
)
test_model = keras.models.load_model(
    "feature_extraction_with_data_augmentation.keras"
)
test_loss, test_acc = test_model.evaluate(test_dataset)
print(f"Test accuracy: {test_acc:.3f}")
### 9. Prepare for Fine-Tuning
conv_base.trainable = True

for layer in conv_base.layers[:-4]:
    layer.trainable = False
    
for layer in conv_base.layers:
    if isinstance(layer, layers.BatchNormalization):
        layer.trainable = False
### 10. Train Model (Fine-Tuning)
model.compile(
    loss="binary_crossentropy",
    optimizer=keras.optimizers.Adam(learning_rate=1e-5),
    metrics=["accuracy"],
)

callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="fine_tuning.keras",
        save_best_only=True,
        monitor="val_loss",
    )
]
history = model.fit(
    augmented_train_dataset,
    epochs=30,
    validation_data=validation_dataset,
    callbacks=callbacks,
)
model = keras.models.load_model("fine_tuning.keras")
test_loss, test_acc = model.evaluate(test_dataset)
print(f"Test accuracy: {test_acc:.3f}")
len(conv_base.trainable_weights)
```
</details>