# Chapter 9. ConvNet architecture patterns

## Table of Contents
- [Residual Connections](#residual-connections)  
- [Batch Normalization](#batch-normalization)  
- [Depthwise Separable Convolutions](#depthwise-separable-convolutions)  
- [ConvNet Architecture Principles](#convnet-architecture-principles)  
- [Vision Transformers (ViTs)](#vision-transformers-vits)  
- [Key Takeaways](#key-takeaways)  
- [Code](#code)  

---

## Residual Connections
![res](./images/09-01.png)

- **Problem**: Deep networks suffer from **vanishing gradients** → training fails  
- **Solution**: Add residual connections  
  - Add input back to output as an **information shortcut**  
  - Ensures gradient flows back to early layers without loss  
- If input and output shapes differ → use `1×1 Conv2D` projection  

---

## Batch Normalization
- Introduced by Ioffe & Szegedy (2015)  
- Normalizes batch statistics (mean, variance) during training  
- Benefits:  
  - Improves gradient flow → enables **deeper networks**  
  - Widely used in ResNet, EfficientNet, Xception, etc.  
- Best Practice:
  ```python
  x = layers.Conv2D(32, 3, use_bias=False)(x)
  x = layers.BatchNormalization()(x)
  x = layers.Activation("relu")(x)
  ```
  → Normalize before activation (better ReLU utilization)  

- **Fine-tuning tip**: Freeze BatchNorm layers (`trainable=False`) for stability  

---

## Depthwise Separable Convolutions
![depthwise](./images/09-02.png)

- Efficient alternative to Conv2D  
- Two steps:  
  - Depthwise convolution (per-channel)  
  - Pointwise convolution (1×1 conv)  
- Advantages:  
  - Fewer parameters, less computation  
  - Faster convergence, less overfitting  
- Used in MobileNet, Xception  

---

## ConvNet Architecture Principles
- Use repeated blocks (Conv → Conv → Pooling)  
- Increase filters as feature maps shrink  
- Deep & narrow > shallow & wide  
- Add residual connections for stable training  
- Use BatchNorm to stabilize gradients  
- Prefer SeparableConv2D for efficiency  

---

## Vision Transformers (ViTs)
- Images split into patches → processed as sequences in Transformer  
- Advantages:  
  - Better at learning **long-range dependencies**  
  - Scales well with large datasets  
- Limitations:  
  - No spatial prior → less data efficient than ConvNets  
  - Require very large datasets (e.g., ImageNet) to be effective  

---

## Key Takeaways
- **Residual connections** = fix vanishing gradients, enable deep training  
- **Batch Normalization** = stabilize gradients, allow deeper models  
- **Depthwise Separable Conv** = fewer parameters, efficient training  
- **Design principles**:  
  - Repeated blocks  
  - More filters as spatial size decreases  
  - Deep & narrow > shallow & wide  
- **ViTs** = powerful for large datasets, but ConvNets are better for small data  

---

## Code
<details>
<summary>Show code</summary>

```python
### Residual Block Implementation
inputs = keras.Input(shape=(32, 32, 3))
x = layers.Rescaling(1.0 / 255)(inputs)

def residual_block(x, filters, pooling=False):
    residual = x
    x = layers.Conv2D(filters, 3, activation="relu", padding="same")(x)
    x = layers.Conv2D(filters, 3, activation="relu", padding="same")(x)
    if pooling:
        x = layers.MaxPooling2D(2, padding="same")(x)
        residual = layers.Conv2D(filters, 1, strides=2)(residual)
    elif filters != residual.shape[-1]:
        residual = layers.Conv2D(filters, 1)(residual)
    x = layers.add([x, residual])
    return x

x = residual_block(x, filters=32, pooling=True)
x = residual_block(x, filters=64, pooling=True)
x = residual_block(x, filters=128, pooling=False)

x = layers.GlobalAveragePooling2D()(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs=inputs, outputs=outputs)
```

```python
''' 
All of these ideas together into a single model
- Your model should be organized into repeated blocks of layers, usually made of multiple convolution layers and a max pooling layer.
- The number of filters in your layers should increase as the size of the spatial feature maps decreases.
- Deep and narrow is better than broad and shallow.
- Introducing residual connections around blocks of layers helps you train deeper networks.
- It can be beneficial to introduce batch normalization layers after your convolution layers.
- It can be beneficial to replace Conv2D layers with SeparableConv2D layers, which are more parameter efficient.
'''
import keras
inputs = keras.Input(shape=(180, 180, 3))
x = layers.Rescaling(1.0 / 255)(inputs)
x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)

for size in [32, 64, 128, 256, 512]:
    residual = x

    x = layers.BatchNormalization()(x)
    x = layers.Activation("relu")(x)
    x = layers.SeparableConv2D(size, 3, padding="same", use_bias=False)(x)

    x = layers.BatchNormalization()(x)
    x = layers.Activation("relu")(x)
    x = layers.SeparableConv2D(size, 3, padding="same", use_bias=False)(x)

    x = layers.MaxPooling2D(3, strides=2, padding="same")(x)

    residual = layers.Conv2D(
        size, 1, strides=2, padding="same", use_bias=False
    )(residual)
    x = layers.add([x, residual])

x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs=inputs, outputs=outputs)
```
</details>  
