# Chapter 9. ConvNet architecture patterns

## Table of Contents

- [Residual Connections](#residual-connections)
- [Batch Normalization](#batch-normalization)
- [Depthwise Separable Convolutions](#depthwise-separable-convolutions)
- [ConvNet Architecture Principles](#convnet-architecture-principles)
- [Vision Transformers (ViTs)](#vision-transformers-vits)
- [내가 이해한 핵심](#내가-이해한-핵심)
- [코드](#코드)

---

## Residual Connections

![res](./images/09-01.png)

* **문제**: 네트워크가 깊어질수록 **vanishing gradient** 발생 → 학습 불가
* **해결책**: Residual connection 추가

  * 입력을 출력에 더해 **정보 shortcut** 제공
  * 기울기(gradient)가 초기 layer까지 손실 없이 전달
* 입력과 출력 shape이 다를 경우 → `1×1 Conv2D`로 projection

---

## Batch Normalization

* 2015년 Ioffe & Szegedy 제안

* 학습 중 배치 단위 평균과 분산을 이용해 정규화

* 효과:

  * Gradient 흐름 개선 → **더 깊은 네트워크 학습 가능**
  * ResNet, EfficientNet, Xception 등 최신 ConvNet에 필수적

* Best Practice:

  ```python
  x = layers.Conv2D(32, 3, use_bias=False)(x)
  x = layers.BatchNormalization()(x)
  x = layers.Activation("relu")(x)
  ```

  → Normalization 후 Activation 적용

* **Fine-tuning 시 주의**: BatchNorm layer는 `trainable=False`로 두는 게 안정적

---

## Depthwise Separable Convolutions

![depthwise](./images/09-02.png)

* Conv2D를 대체할 수 있는 **효율적 레이어**
* 과정:

  * Depthwise Conv → 채널별로 독립적 spatial convolution
  * Pointwise Conv (1×1) → 채널 통합
* 장점:

  * 파라미터 수 ↓, 연산량 ↓
  * 학습 속도 ↑, 과적합 ↓
* MobileNet, Xception 등에서 활용

---

## ConvNet Architecture Principles

* Layer는 반복적 블록 구조로 설계 (Conv → Conv → Pooling)
* Feature map 크기가 줄어들수록 **필터 수 증가**
* 얕고 넓은 구조보다 → **깊고 좁은 구조** 선호
* Residual connections → 깊은 네트워크 안정적 학습
* Batch Normalization → 안정적 gradient 흐름
* Conv2D 대신 SeparableConv2D → 효율성↑

---

## Vision Transformers (ViTs)

* 이미지를 **패치 단위 시퀀스**로 변환 → Transformer로 처리
* 장점:

  * 장거리 의존성(Long-range dependencies) 학습 우수
  * 대규모 데이터에서 효과적
* 단점:

  * ConvNet의 2D 공간 prior 없음 → **데이터 효율성 낮음**
  * 작은 데이터셋에서는 ConvNet이 더 적합

---

## 내가 이해한 핵심

* **Residual connections** = Vanishing gradient 해결, 깊은 네트워크 학습 가능
* **Batch Normalization** = Gradient 안정화, 깊은 네트워크 학습 촉진
* **Depthwise Separable Conv** = 효율적 Conv, 적은 파라미터로 비슷한 성능
* **좋은 ConvNet 설계 원칙**

  * 반복 블록 구조
  * Feature map 줄어들수록 필터 수 증가
  * Deep & Narrow > Shallow & Wide
* **ViTs** = 대규모 데이터셋에서 강력하지만, 소규모에서는 ConvNet이 더 효과적

---

## 코드
<details>
<summary>코드 보기</summary>

```python
### Residual Block Implementation
inputs = keras.Input(shape=(32, 32, 3))
x = layers.Rescaling(1.0 / 255)(inputs)

def residual_block(x, filters, pooling=False):
    residual = x
    x = layers.Conv2D(filters, 3, activation="relu", padding="same")(x)
    x = layers.Conv2D(filters, 3, activation="relu", padding="same")(x)
    if pooling:
        x = layers.MaxPooling2D(2, padding="same")(x)
        residual = layers.Conv2D(filters, 1, strides=2)(residual)
    elif filters != residual.shape[-1]:
        residual = layers.Conv2D(filters, 1)(residual)
    x = layers.add([x, residual])
    return x

x = residual_block(x, filters=32, pooling=True)
x = residual_block(x, filters=64, pooling=True)
x = residual_block(x, filters=128, pooling=False)

x = layers.GlobalAveragePooling2D()(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs=inputs, outputs=outputs)
```

```python
''' 
All of these ideas together into a single model
- Your model should be organized into repeated blocks of layers, usually made of multiple convolution layers and a max pooling layer.
- The number of filters in your layers should increase as the size of the spatial feature maps decreases.
- Deep and narrow is better than broad and shallow.
- Introducing residual connections around blocks of layers helps you train deeper networks.
- It can be beneficial to introduce batch normalization layers after your convolution layers.
- It can be beneficial to replace Conv2D layers with SeparableConv2D layers, which are more parameter efficient.
'''
import keras
inputs = keras.Input(shape=(180, 180, 3))
x = layers.Rescaling(1.0 / 255)(inputs)
x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)

for size in [32, 64, 128, 256, 512]:
    residual = x

    x = layers.BatchNormalization()(x)
    x = layers.Activation("relu")(x)
    x = layers.SeparableConv2D(size, 3, padding="same", use_bias=False)(x)

    x = layers.BatchNormalization()(x)
    x = layers.Activation("relu")(x)
    x = layers.SeparableConv2D(size, 3, padding="same", use_bias=False)(x)

    x = layers.MaxPooling2D(3, strides=2, padding="same")(x)

    residual = layers.Conv2D(
        size, 1, strides=2, padding="same", use_bias=False
    )(residual)
    x = layers.add([x, residual])

x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs=inputs, outputs=outputs)
```

</details> 