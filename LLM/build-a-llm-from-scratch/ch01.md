# Chapter 1. Understanding Large Language Models

## Table of Contents
- [Pretraining과 Fine-tuning](#pretraining과-fine-tuning)
- [Transformer 구조](#transformer-구조)
- [GPT 아키텍처](#gpt-아키텍처)
- [LLM 개발의 세 단계](#llm-개발의-세-단계)
- [내가 이해한 핵심](#내가-이해한-핵심)
- [📖 원문 발췌](#-원문-발췌)

---

LLM(Large Language Model)은 **인간 언어를 이해하고 생성하는 신경망 기반 모델**이다. 가장 중요한 특징은 방대한 텍스트를 학습하여 문맥을 이해하고, 새로운 텍스트를 만들어낼 수 있다는 점이다.

---

## Pretraining과 Fine-tuning

![pre-training and fine-tuning](./images/1-3.png)

LLM은 먼저 **사전학습(Pretraining)** 단계를 거친다.  
이 단계에서는 방대한 텍스트 데이터를 활용하여 다음 단어를 예측하도록 학습한다.  
- *"raw text"* = Label이 없는 일반 텍스트 데이터  
- 결과물 = **pretrained LLM** 또는 **foundation model**

그다음에는 **파인튜닝(Fine-tuning)** 단계가 뒤따른다. Label이 포함된 데이터셋을 활용하여 특정 태스크에 맞게 조정한다.

### Fine-tuning의 두 가지 대표 방식
- **Instruction fine-tuning**: 질문과 답변 쌍으로 학습 (예: 번역 요청 → 정답 번역)
- **Classification fine-tuning**: 텍스트와 레이블 쌍으로 학습 (예: 이메일 → “spam / not spam”)

---

## Transformer 구조

![original transformer](./images/1-4.png)

현대 LLM의 핵심은 **Transformer 아키텍처**(Vaswani et al., 2017, *Attention Is All You Need*)이다.  
트랜스포머는 두 부분으로 구성된다.

- **Encoder**: 입력 텍스트를 벡터로 변환 (문맥 정보 인코딩)  
- **Decoder**: 이 벡터를 바탕으로 출력 텍스트 생성  

핵심 메커니즘은 **Self-Attention**이다.  
이는 입력 시퀀스 내 단어들 간의 중요도를 동적으로 계산하여 **장거리 의존성**과 **문맥적 관계**를 잘 학습하게 한다.

---

## GPT 아키텍처

![GPT](./images/1-8.png)

GPT는 트랜스포머의 **Decoder 부분만 사용하는 구조**이다.  
왼쪽에서 오른쪽으로 순차적으로 단어를 생성하며,  
텍스트 생성 및 다음 단어 예측에 최적화되어 있다.  
즉, 한 단어씩 예측하면서 문장을 완성해 나간다.

---

## LLM 개발의 세 단계

![LLM stages](./images/1-9.png)

LLM을 만들고 활용하는 과정은 크게 세 단계로 나눌 수 있다.

1. **아키텍처 구현 & 데이터 준비**  
   - Transformer 기반 LLM 구조 정의  
   - 학습용 텍스트 데이터 전처리  
2. **사전학습 (Pretraining)**  
   - 방대한 데이터로 언어 일반 능력을 습득  
   - 결과물: Foundation Model  
3. **파인튜닝 (Fine-tuning)**  
   - 특정 업무에 맞게 조정  
   - 개인 비서형 LLM, 분류기 등 다양한 형태로 발전  

---

## 내가 이해한 핵심

- **Pretraining** = 언어의 일반적 패턴 학습  
- **Fine-tuning** = 특정 태스크 맞춤 학습  
- **Transformer** = Encoder + Decoder 구조  
- **GPT** = Decoder만 활용 → 텍스트 생성에 특화  
- **LLM 개발 단계** = 아키텍처 구현 → 사전학습 → 파인튜닝

---

## 📖 원문 발췌

<details>
<summary>원문 보기</summary>
An LLM is a neural network designed to understand, generate, and respond to human-like text. 

The first step in creating an LLM is to train it on a large corpus of text data, sometimes referred to as raw text. Here, *"raw"* refers to the fact that this data is just regular text without any labeling information.

This first training stage of an LLM is also known as **pretraining**, creating an initial pretrained LLM, often called a *base* or *foundation model*. 

After obtaining a pretrained LLM by training on large text datasets, where the LLM is trained to predict the next word in the text, we can further train the LLM on labeled data, also known as **fine-tuning**.

The two most popular categories of fine-tuning LLMs are **instruction fine-tuning** and **classification fine-tuning**. In instruction fine-tuning, the labeled dataset consists of instruction and answer pairs, such as a query to translate a text accompanied by the correctly translated text. In classification fine-tuning, the labeled dataset consists of texts and associated class labels—for example, emails associated with “spam” and “not spam” labels.

Most modern LLMs rely on the transformer architecture, which is a deep neural network architecture introduced in the 2017 paper “Attention Is All You Need” (https://arxiv.org/abs/1706.03762).

The transformer architecture consists of two submodules: an *encoder* and a *decoder*. The encoder module processes the input text and encodes it into a series of numerical representations or vectors that capture the contextual information of the input. Then, the decoder module takes these encoded vectors and generates the output text. 

A key component of transformers and LLMs is the **self-attention mechanism**, which allows the model to weigh the importance of different words or tokens in a sequence relative to each other. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data, enhancing its ability to generate coherent and contextually relevant output.

The GPT architecture employs only the decoder portion of the original transformer. It is designed for unidirectional, left-to-right processing, making it well suited for text generation and next-word prediction tasks to generate text in an iterative fashion, one word at a time.

The three main stages of coding an LLM are implementing the LLM architecture and data preparation process (stage 1), pretraining an LLM to create a foundation model (stage 2), and fine-tuning the foundation model to become a personal assistant or text classifier (stage 3).
</details>