# Chapter 1. Understanding Large Language Models

## Table of Contents
- [Pretraining과 Fine-tuning](#pretraining과-fine-tuning)
- [Transformer 구조](#transformer-구조)
- [GPT 아키텍처](#gpt-아키텍처)
- [LLM 개발의 세 단계](#llm-개발의-세-단계)
- [내가 이해한 핵심](#내가-이해한-핵심)

---

LLM(Large Language Model)은 **인간 언어를 이해하고 생성하는 신경망 기반 모델**이다. 가장 중요한 특징은 방대한 텍스트를 학습하여 문맥을 이해하고, 새로운 텍스트를 만들어낼 수 있다는 점이다.

---

## Pretraining과 Fine-tuning

![pre-training and fine-tuning](./images/1-3.png)

LLM은 먼저 **사전학습(Pretraining)** 단계를 거친다.  
이 단계에서는 방대한 텍스트 데이터를 활용하여 다음 단어를 예측하도록 학습한다.  
- *"raw text"* = Label이 없는 일반 텍스트 데이터  
- 결과물 = **pretrained LLM** 또는 **foundation model**

그다음에는 **파인튜닝(Fine-tuning)** 단계가 뒤따른다. Label이 포함된 데이터셋을 활용하여 특정 태스크에 맞게 조정한다.

### Fine-tuning의 두 가지 대표 방식
- **Instruction fine-tuning**: 질문과 답변 쌍으로 학습 (예: 번역 요청 → 정답 번역)
- **Classification fine-tuning**: 텍스트와 레이블 쌍으로 학습 (예: 이메일 → “spam / not spam”)

---

## Transformer 구조

![original transformer](./images/1-4.png)

현대 LLM의 핵심은 **Transformer 아키텍처**(Vaswani et al., 2017, *Attention Is All You Need*)이다.  
트랜스포머는 두 부분으로 구성된다.

- **Encoder**: 입력 텍스트를 벡터로 변환 (문맥 정보 인코딩)  
- **Decoder**: 이 벡터를 바탕으로 출력 텍스트 생성  

핵심 메커니즘은 **Self-Attention**이다.  
이는 입력 시퀀스 내 단어들 간의 중요도를 동적으로 계산하여 **장거리 의존성**과 **문맥적 관계**를 잘 학습하게 한다.

---

## GPT 아키텍처

![GPT](./images/1-8.png)

GPT는 트랜스포머의 **Decoder 부분만 사용하는 구조**이다.  
왼쪽에서 오른쪽으로 순차적으로 단어를 생성하며,  
텍스트 생성 및 다음 단어 예측에 최적화되어 있다.  
즉, 한 단어씩 예측하면서 문장을 완성해 나간다.

---

## LLM 개발의 세 단계

![LLM stages](./images/1-9.png)

LLM을 만들고 활용하는 과정은 크게 세 단계로 나눌 수 있다.

1. **아키텍처 구현 & 데이터 준비**  
   - Transformer 기반 LLM 구조 정의  
   - 학습용 텍스트 데이터 전처리  
2. **사전학습 (Pretraining)**  
   - 방대한 데이터로 언어 일반 능력을 습득  
   - 결과물: Foundation Model  
3. **파인튜닝 (Fine-tuning)**  
   - 특정 업무에 맞게 조정  
   - 개인 비서형 LLM, 분류기 등 다양한 형태로 발전  

---

## 내가 이해한 핵심

- **Pretraining** = 언어의 일반적 패턴 학습  
- **Fine-tuning** = 특정 태스크 맞춤 학습  
- **Transformer** = Encoder + Decoder 구조  
- **GPT** = Decoder만 활용 → 텍스트 생성에 특화  
- **LLM 개발 단계** = 아키텍처 구현 → 사전학습 → 파인튜닝