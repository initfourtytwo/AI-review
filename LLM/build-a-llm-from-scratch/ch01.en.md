# Chapter 1. Understanding Large Language Models

## Table of Contents
- [Pretraining and Fine-tuning](#pretraining-and-fine-tuning)
- [Transformer Architecture](#transformer-architecture)
- [GPT Architecture](#gpt-architecture)
- [Three Stages of LLM Development](#three-stages-of-llm-development)
- [Key Takeaways](#key-takeaways)

---

A Large Language Model (LLM) is a **neural network designed to understand and generate human-like text**.  
Its key feature is learning from massive amounts of text data to understand context and generate new text accordingly.

---

## Pretraining and Fine-tuning

![pre-training and fine-tuning](./images/1-3.png)

The first step is **pretraining**.  
In this stage, the model is trained to predict the next word given large amounts of unlabeled *raw text*.  
- *Raw text* = plain text without labels  
- Result = **pretrained LLM** or **foundation model**

The next step is **fine-tuning**, where the pretrained model is adapted to specific tasks using labeled datasets.  

### Two common fine-tuning methods
- **Instruction fine-tuning**: trained on instruction–response pairs (e.g., request: translate → response: correct translation).  
- **Classification fine-tuning**: trained on text–label pairs (e.g., emails → “spam” or “not spam”).  

---

## Transformer Architecture

![original transformer](./images/1-4.png)

Modern LLMs are based on the **Transformer architecture** (Vaswani et al., 2017, *Attention Is All You Need*).  
It consists of two components:

- **Encoder**: converts input text into contextual representations (vectors).  
- **Decoder**: generates output text from these representations.  

A core mechanism is **Self-Attention**, which dynamically weighs the importance of words relative to each other.  
This allows the model to capture **long-range dependencies** and **contextual relationships**.  

---

## GPT Architecture

![GPT](./images/1-8.png)

GPT uses **only the Decoder** part of the Transformer.  
It generates text **left-to-right, one word at a time**, making it well-suited for text generation and next-word prediction.  

---

## Three Stages of LLM Development

![LLM stages](./images/1-9.png)

Developing and applying an LLM can be summarized in three stages:

1. **Implement architecture & prepare data**  
   - Define the Transformer-based LLM  
   - Preprocess training data  
2. **Pretraining**  
   - Train on massive text data to learn general language patterns  
   - Result: Foundation model  
3. **Fine-tuning**  
   - Adapt to specific tasks  
   - Examples: assistant-style LLM, text classifier, etc.  

---

## Key Takeaways

- **Pretraining** = learning general language patterns  
- **Fine-tuning** = adapting to specific tasks  
- **Transformer** = Encoder + Decoder with Self-Attention  
- **GPT** = Decoder-only, specialized for text generation  
- **LLM pipeline** = Architecture → Pretraining → Fine-tuning  
