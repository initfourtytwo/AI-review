# Week 1 ‚Äî Build Your First LLM Product: Exploring Top Models & Transformers

## Table of Contents

1. [Core Competencies of an LLM Engineer](#core-competencies-of-an-llm-engineer)
2. [Frontier vs Open-Source Models](#frontier-vs-open-source-models)
3. [Tokenization & Context Windows](#tokenization--context-windows)
4. [Working with APIs and Local Models](#working-with-apis-and-local-models)
5. [System & User Prompts](#system--user-prompts)
6. [Key Takeaways](#key-takeaways)
7. [Code](#code)

---

## Core Competencies of an LLM Engineer

* **Model understanding**:
  Learn architectures, modalities (text, image, audio), and trade-offs between open-source and closed-source models.
* **Tooling stack**:

  * **Hugging Face** (model management, tokenizers)
  * **LangChain** (agentic composition and glue logic)
  * **Gradio** (quick UI prototyping)
  * **Weights & Biases** (experiment tracking)
  * **Modal** (deployment at scale)
* **Techniques**: Retrieval-Augmented Generation (RAG), fine-tuning, and multi-agent orchestration.
* **Mindset**: Combine experimentation with systematic benchmarking and iteration.

---

## Frontier vs Open-Source Models

* **Frontier models**: GPT-4 (001 Preview), Claude 3.5, Gemini 1.5 Pro ‚Äî closed, highly capable but paid.
* **Open-source models**: Llama 3.2, Gemma, Qwen 2.5, Mistral 7B ‚Äî flexible, self-hostable, often less performant.
* **Practical exercise**: Compare performance, speed, and reasoning differences by running each locally via **Ollama**.
* **Insight**: Model selection depends on trade-offs among **capability, latency, privacy, and cost** ‚Äî a critical engineering judgment skill.

---

## Tokenization & Context Windows

* **Tokens**: Basic input units LLMs process (‚âà 4 characters or 0.75 words per token).
* **Rule of thumb**: 1 000 tokens ‚âà 750 English words.
* **Tokenizer behavior**:

  * Common words map 1:1 to tokens.
  * Rare/compound words (e.g., *handcrafted*) split into sub-tokens ( ‚Äúhand‚Äù + ‚Äúcrafted‚Äù ).
  * Numeric sequences (like œÄ) chunked in 3-digit groups.
* **Context window**: Max number of tokens a model can attend to when predicting the next one.
  Larger windows ‚Üí better long-context reasoning, but higher cost and memory load.

---

## Working with APIs and Local Models

* **Three ways to use LLMs**:

  1. **Cloud API** (OpenAI / Anthropic / Gemini ‚Äì pay-per-use, scalable).
  2. **Open-source code access** via Hugging Face + PyTorch (on Colab GPU).
  3. **Local compiled models** using Ollama (C++ optimized inference, privacy-friendly).
* **Trade-off**: Cloud = control + cost ‚Äì privacy; Local = privacy + speed ‚Äì flexibility.

---

## System & User Prompts

* **System prompt**: Defines persona, tone, and rules (e.g., ‚Äúsarcastic‚Äù, ‚Äúreply in Spanish‚Äù).
* **User prompt**: The actual instruction or question.
* **Multi-model chaining**: Example ‚Äî generate content in English then call another model for translation.
* **Why it matters**: Practicing structured prompt design and multi-call flows lays groundwork for agentic AI.

---

## Key Takeaways

* ‚Äú**Learn by doing**‚Äù is central ‚Äî each concept is immediately applied in code.
* Understanding **tokens and context windows** is essential for scaling and cost management.
* Comparing **frontier vs open-source models** develops critical evaluation skills.
* **Prompt engineering + API orchestration** forms the core of agentic AI thinking.

---

## Code

<details>
<summary>Show Code</summary>

``` python
# ============================================================
# üß† Week 1 ‚Äî Essential Code Patterns
# ============================================================

# (1) Core LLM Call
# Basic LLM call structure ‚Äî define messages ‚Üí call model ‚Üí extract output.
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Summarize this article in markdown."}
]
resp = client.chat.completions.create(model="gpt-4o-mini", messages=messages)
print(resp.choices[0].message.content)

# ------------------------------------------------------------

# (2) System / User Prompt Structure
# Separate system (rules, tone) from user (task instruction).
# Keeps prompts modular and predictable.
SYSTEM_PROMPT = "You are an expert summarizer. Return concise markdown."
USER_PROMPT = f"Summarize:\n\n{raw_text}"
messages = [
    {"role": "system", "content": SYSTEM_PROMPT},
    {"role": "user", "content": USER_PROMPT}
]

# ------------------------------------------------------------

# (3) Summarization Helper
# Reusable utility for extracting key points ‚Äî used in many workflows.
def summarize(text):
    msgs = [
        {"role": "system", "content": "Summarize key ideas in markdown bullets."},
        {"role": "user", "content": text}
    ]
    return client.chat.completions.create(
        model="gpt-4o-mini", messages=msgs
    ).choices[0].message.content

# ------------------------------------------------------------

# (4) Streaming Render
# Displays token-by-token output in real time, useful for debugging and demos.
from IPython.display import Markdown, display
view, buf = display(Markdown(""), display_id=True), ""
for chunk in stream:      # chunk = partial text/token from LLM
    buf += chunk
    view.update(Markdown(buf))

# ------------------------------------------------------------

# (5) Generate ‚Üí Transform Pipeline
# Two simple LLM calls: create ‚Üí transform
# Generate: produce a first version (summary, report, explanation).
# Transform: refine, translate, or reformat it.
# This two-step workflow adds structure and control ‚Äî the basis of later multi-agent systems.
draft = summarize(source_text)  # Generate
final = client.chat.completions.create(  # Transform
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "Translate this to Spanish and keep markdown."},
        {"role": "user", "content": draft}
    ]
).choices[0].message.content

# Result: A controlled two-step workflow ‚Äî generate content, then adapt or translate it.
# ============================================================
```

</details>