# Week 3 â€” Open Source Foundations: Hugging Face, Pipelines, Tokenizers & Models

## ðŸ§­ Table of Contents

1. [Pipelines â€” High-Level Inference API](#pipelines--high-level-inference-api)
2. [Tokenizers â€” Text-to-Tokens Translation](#tokenizers--text-to-tokens-translation)
3. [Models â€” Running and Comparing LLMs](#models--running-and-comparing-llms)
4. [Key Takeaways](#key-takeaways)
5. [Code](#code)

---

## Pipelines â€” High-Level Inference API

* **Concept:** One-line access to pre-trained models for common ML tasks.
* **Structure:**

  ```python
  from transformers import pipeline
  nlp = pipeline("sentiment-analysis")
  nlp("I'm thrilled to learn LLM engineering!")
  ```
* **Common Pipelines:**

  * `sentiment-analysis`
  * `text-classification`, `ner`, `qa`
  * `summarization`, `translation`
  * `text-generation`, `image-generation`, `text-to-speech`
* **GPU usage:** Add `device="cuda"` for acceleration.
* **Key Advantage:** Unified interface â†’ plug-and-play tasks with any model.

---

## Tokenizers â€” Text-to-Tokens Translation

* **Definition:** Converts text â†” numerical tokens.
* **Core Methods:**

  * `encode(text)` â†’ list of token IDs.
  * `decode(tokens)` â†’ readable text.
* **Special Tokens:**

  * `<s>` / `<bos>` : beginning of sequence
  * `</s>` / `<eos>` : end of sequence
  * `<pad>`, `<unk>`, `<system>`, `<user>`, `<assistant>`
* **Chat Templates:**
  Standardize message formatting across roles:

  ```python
  messages = [
      {"role": "system", "content": "You are helpful."},
      {"role": "user", "content": "Tell me a joke."}
  ]
  tokenizer.apply_chat_template(messages, tokenize=False)
  ```
* **Observation:** Each model family (e.g., LLaMA, Phi, Qwen) uses distinct special tokens â€” mismatching tokenizers causes invalid inputs.

---

## Models â€” Running and Comparing LLMs

* **Definition:** Actual Transformer-based neural network performing inference.
* **Class:** `AutoModelForCausalLM.from_pretrained()`
* **Quantization:**

  * Reduces model precision (e.g., 32 â†’ 8 â†’ 4 bits).
  * **Trade-off:** Slightly lower accuracy, drastically less memory.
  * Managed via `BitsAndBytesConfig` (library: `bitsandbytes`).

---

## Key Takeaways

* **Pipelines** simplify inference; **Tokenizers** define language structure; **Models** provide computation.
* **Quantization** enables large models on limited hardware.

---

## Code

<details>
<summary>Show Code</summary>

```python
# ============================================================
# ðŸ§  Week 3
# ============================================================

# (1) Pipeline Example
from transformers import pipeline
sentiment = pipeline("sentiment-analysis", device="cuda")
print(sentiment("I love open-source LLMs!"))

# ------------------------------------------------------------

# (2) Tokenizer Example
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B")
tokens = tokenizer.encode("Hello LLM Engineers!")
print(tokens, "â†’", tokenizer.decode(tokens))

# ------------------------------------------------------------

# (3) Chat Template
messages = [
    {"role": "system", "content": "You are helpful."},
    {"role": "user", "content": "Tell a data-science joke."}
]
print(tokenizer.apply_chat_template(messages, tokenize=False))

# ------------------------------------------------------------

# (4) Quantized Model Loading
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
bnb_cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True)
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Meta-Llama-3-8B",
    quantization_config=bnb_cfg,
    device_map="auto"
)

# ------------------------------------------------------------

# (5) Generation + Streaming
from transformers import TextStreamer
streamer = TextStreamer(tokenizer)
inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")
model.generate(inputs, max_new_tokens=80, streamer=streamer)

# ------------------------------------------------------------
============================================================
```

</details>
