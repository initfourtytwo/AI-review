# Week 3 — Open Source Foundations: Hugging Face, Pipelines, Tokenizers & Models

## 🧭 목차

1. [Pipeline — 고수준 추론 API](#pipeline--고수준-추론-api)
2. [Tokenizer — 텍스트와 토큰의 변환](#tokenizer--텍스트와-토큰의-변환)
3. [모델 — LLM 실행 및 비교](#모델--llm-실행-및-비교)
4. [핵심 요약](#핵심-요약)
5. [코드](#코드)

---

## Pipeline — 고수준 추론 API

* **개념:** 사전 학습된 모델을 단 한 줄로 불러와 다양한 머신러닝 작업을 수행할 수 있는 인터페이스.

* **기본 구조:**

  ```python
  from transformers import pipeline
  nlp = pipeline("sentiment-analysis")
  nlp("I'm thrilled to learn LLM engineering!")
  ```

* **자주 쓰이는 파이프라인 종류:**

  * `sentiment-analysis` (감정 분석)
  * `text-classification`, `ner`, `qa` (분류, 개체명 인식, 질의응답)
  * `summarization`, `translation` (요약, 번역)
  * `text-generation`, `image-generation`, `text-to-speech` (생성형 모델)

* **GPU 활용:** `device="cuda"` 옵션으로 GPU 가속 가능.

* **핵심 장점:** 다양한 모델을 **플러그 앤 플레이** 형태로 바로 사용 가능.

---

## Tokenizer — 텍스트와 토큰의 변환

* **정의:** 텍스트를 모델이 이해할 수 있는 **숫자 토큰**으로 변환하고, 다시 사람이 읽을 수 있는 텍스트로 복원.

* **주요 메서드:**

  * `encode(text)` → 토큰 ID 리스트
  * `decode(tokens)` → 텍스트 복원

* **특수 토큰:**

  * `<s>` / `<bos>` : 문장의 시작
  * `</s>` / `<eos>` : 문장의 끝
  * `<pad>`, `<unk>`, `<system>`, `<user>`, `<assistant>`

* **Chat Template:**
  대화형 모델의 시스템·사용자 메시지를 일정한 포맷으로 정리

  ```python
  messages = [
      {"role": "system", "content": "You are helpful."},
      {"role": "user", "content": "Tell me a joke."}
  ]
  tokenizer.apply_chat_template(messages, tokenize=False)
  ```

* **주의할 점:**
  모델별로 **토크나이저가 다름** (예: LLaMA, Phi, Qwen 등).
  다른 토크나이저를 섞어 쓰면 입력 포맷이 깨짐.

---

## 모델 — LLM 실행 및 비교

* **정의:** 실제 추론(생성)을 수행하는 **Transformer 신경망**.

* **클래스:** `AutoModelForCausalLM.from_pretrained()`

* **양자화(Quantization):**

  * 모델의 수치 정밀도를 줄여 (예: 32bit → 8bit → 4bit)
    **메모리 사용량과 비용 절감**
  * 약간의 정확도 손실이 있을 수 있음
  * `BitsAndBytesConfig`로 관리 (`bitsandbytes` 라이브러리 사용)

---

## 핵심 요약

* **Pipeline**은 추론을 간소화하고, **Tokenizer**는 언어 구조를 정의하며, **Model**은 실제 연산을 수행한다.
* **양자화(Quantization)**를 통해 대형 모델도 제한된 하드웨어에서 실행 가능하다.

---

## 코드

<details>
<summary>코드 보기</summary>

```python
# ============================================================
# 🧠 Week 3
# ============================================================

# (1) Pipeline 예시
from transformers import pipeline
sentiment = pipeline("sentiment-analysis", device="cuda")
print(sentiment("I love open-source LLMs!"))

# ------------------------------------------------------------

# (2) Tokenizer 예시
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B")
tokens = tokenizer.encode("Hello LLM Engineers!")
print(tokens, "→", tokenizer.decode(tokens))

# ------------------------------------------------------------

# (3) Chat Template
messages = [
    {"role": "system", "content": "You are helpful."},
    {"role": "user", "content": "Tell a data-science joke."}
]
print(tokenizer.apply_chat_template(messages, tokenize=False))

# ------------------------------------------------------------

# (4) 양자화 모델 로딩
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
bnb_cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True)
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Meta-Llama-3-8B",
    quantization_config=bnb_cfg,
    device_map="auto"
)

# ------------------------------------------------------------

# (5) 생성 + 스트리밍
from transformers import TextStreamer
streamer = TextStreamer(tokenizer)
inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")
model.generate(inputs, max_new_tokens=80, streamer=streamer)

# ============================================================
```

</details>
