# Week 2 ‚Äî Build a Multi-Modal Chatbot: LLMs, Gradio UI, and Agents in Action

## üß≠ Table of Contents

1. [Gradio for Rapid UI Prototyping](#gradio-for-rapid-ui-prototyping)
2. [Chatbots, Context, and Multi-Shot Prompting](#chatbots-context-and-multi-shot-prompting)
3. [Tools and Function Calling](#tools-and-function-calling)
4. [Agents and Multimodal Assistants](#agents-and-multimodal-assistants)
5. [Key Takeaways](#key-takeaways)
6. [Code](#code)

---

## Gradio for Rapid UI Prototyping

* **Purpose:** Build functional interfaces for LLMs in seconds.
* **Core Pattern:**

  ```python
  import gradio as gr
  def greet(name): return f"Hello, {name}!"
  gr.Interface(fn=greet, inputs="text", outputs="text").launch()
  ```

---

## Chatbots, Context, and Multi-Shot Prompting

* **Objective:** Build a **customer-support assistant** with Gradio‚Äôs `ChatInterface`.
* **Conversation memory:** Each user‚Äìassistant pair is re-sent with every API call (no hidden memory).
* **Prompt strategy:**

  * **System message** defines persona (‚Äúhelpful sales assistant‚Äù).
  * **One-shot / Multi-shot examples** guide tone and factual grounding.
* **Dynamic context injection:**

  * Detect keywords (e.g., ‚Äúbelt‚Äù) ‚Üí append new system messages.
  * Simulates mini-RAG by adding domain knowledge at runtime.
* **Lesson:** Context = all prior messages + system rules + any injected info.

---

## Tools and Function Calling

* **Definition:** Tools let LLMs **invoke external functions** (e.g., calculator, database lookup).
* **Workflow:**

  1. Define a Python function (e.g., `get_ticket_price(city)`).
  2. Describe it in a JSON schema (`tools=[{name, description, parameters}]`).
  3. Pass `tools` into the API call.
  4. If the LLM requests a tool, execute it and return results via a `"tool"` role message.
* **Example:** Airline assistant using `get_ticket_price()` as a callable tool to fetch fares.
* **Conceptual parallel:** Like earlier context injection‚ÄîLLM simply asks for data and we feed results back.

---

## Agents and Multimodal Assistants

* **Definition:** Agents = autonomous or specialized components that perform subtasks (e.g., image creation, speech).
* **Implemented agents:**

  * **Artist agent:** uses DALL-E 3 to generate destination images.
  * **Talker agent:** uses TTS (text-to-speech) to read replies.
* **Integration:** Extend chat logic ‚Üí when a tool is called, also invoke artist and talker.
* **Framework idea:**

  * Tools = functions.
  * Agents = functions + purpose + autonomy + modality.
  * Combined ‚Üí **multimodal agentic AI** system.

---

## Key Takeaways

* Frontier APIs share a **common structure**; differences are minor syntax.
* **Gradio** enables instant deployment and collaboration for LLM prototypes.
* **Chat UIs** rely on full-history prompts; ‚Äúmemory‚Äù is illusion built via context.
* **Tools** = controlled function calling; foundation for **action-taking models**.
* **Agents** = orchestration of multiple LLMs and modalities for real-world tasks.
* Hands-on progression: **API ‚Üí UI ‚Üí Chatbot ‚Üí Tool ‚Üí Agentic App**.

---

## Code

<details>
<summary>Show Code</summary>

```python
# ============================================================
# üß† Week 2 ‚Äî Core Code Patterns
# ============================================================

# (1) Multi-Model API Calls
# Unified call style for OpenAI, Anthropic, Gemini.
messages = [{"role":"system","content":"You are a witty assistant."},
            {"role":"user","content":"Tell a data-science joke."}]
resp = client.chat.completions.create(model="gpt-4o-mini", messages=messages)
print(resp.choices[0].message.content)

# ------------------------------------------------------------

# (2) Gradio UI Wrapper
# Minimal UI for LLM calls.
import gradio as gr
def message_gpt(prompt):
    msgs=[{"role":"system","content":"You are helpful."},
          {"role":"user","content":prompt}]
    return client.chat.completions.create(model="gpt-4o-mini", messages=msgs)\
                .choices[0].message.content
gr.Interface(fn=message_gpt, inputs="text", outputs="markdown").launch()

# ------------------------------------------------------------

# (3) Streaming Generator
# Streams cumulative output for live UI rendering.
def stream_gpt(prompt):
    msgs=[{"role":"system","content":"Helpful assistant."},
          {"role":"user","content":prompt}]
    result=""
    for chunk in client.chat.completions.create(
            model="gpt-4o-mini", messages=msgs, stream=True):
        if chunk.choices[0].delta.content:
            result += chunk.choices[0].delta.content
            yield result

# ------------------------------------------------------------

# (4) Tool Integration
# Airline price lookup demo.
def get_ticket_price(city):
    prices={"london":799,"paris":899,"tokyo":1400,"berlin":499}
    return prices.get(city.lower(),"unknown")

tools=[{
  "type":"function",
  "function":{
    "name":"get_ticket_price",
    "description":"Return ticket price for a city.",
    "parameters":{"type":"object",
      "properties":{"destination_city":{"type":"string"}},
      "required":["destination_city"]}
}}]

# LLM call with tool access
resp = client.chat.completions.create(
    model="gpt-4o-mini", messages=messages, tools=tools)

# ------------------------------------------------------------

# (5) Agentic Extension
# Agents for image + voice responses.
def artist(city):
    return openai.images.generate(model="dall-e-3",
        prompt=f"Scenic view of {city} for airline brochure.",
        size="512x512").data[0].b64_json

def talker(text):
    speech = openai.audio.speech.create(model="gpt-4o-mini-tts", input=text)
    with open("speech.mp3","wb") as f: f.write(speech.audio)

# Combined assistant: chat + image + voice
def chat_agent(prompt, history=[]):
    reply = message_gpt(prompt)
    talker(reply)
    if "to" in prompt.lower(): artist(prompt.split("to")[-1].strip())
    return reply

# ============================================================
```

</details>
